\documentclass[10pt,a4paper]{article}
\usepackage[spanish]{babel}%corta palabras en español
\usepackage[latin1]{inputenc}%escribir con acentos, ñ
\usepackage{graphicx}
\begin{document}
$$ \mbox{\bf Universidad de Buenos Aires} $$
$$ \mbox{Facultad de Ciencias Exáctas y Naturales} $$
$$ \mbox{(FCEyN)} $$
$$ \mbox{Departamento de Computación} $$

{\large Trabajo Práctico Número 1: Espía por error (numérico)}

\medskip


{González Sergio (gonzalezsergio2003@yahoo.com.ar)}

{González Emiliano (XJesse\_JamesX@hotmail.com)}

{Ferro Mariano(eltrencitomasverde@gmail.com)}

\medskip
Palábras clave:

Resúmen:
\medskip

\pagebreak
\leftline{\bf Introducción:}


El análisis numérico básicamente se encarga de analizar, describir y crear algoritmos numéricos que permiten resolver problemas matemáticos. Estos algoritmos generalmente nos permiten obtener resultados aproximados, ya que contienen un número finito de pasos. El uso del análisis numérico toma gran importancia con el uso de las computadoras y el poder de cálculo que ellas tienen. Por este medio, es posible resolver problemas más complejos.

Pero el uso de computadoras para hacer cálculos complejos trae un problema consigo, y surge el concepto de error. Este concepto nace debido a que las computadoras trabajan con un rango finito de números, y además cada uno de estos está representado de una forma también finita.


Los errores están divididos en tres tipos: Errores en los datos de entrada, errores de redondeo y errores de truncamiento. Los errores en los datos de entrada no están causados por el algoritmo que resuelve el problema, sino por valores que inician el algoritmo, generalmente estos valores se refieren a mediciones o magnitudes físicas. Los errores de redondeo surgen cuando se utilizan operaciones que tienen una representación numérica finita, esto significa que tienen una precisión limitada con respecto al resultado que devuelven. Y los errores de truncamiento están relacionados con el algoritmo en si, esto quiere decir que dependen de la forma en que se resuelve el problema, en algunos casos el error de truncamiento se puede disminuir modificando o refinando el algoritmo, esto generalmente implica aumentar la cantidad de
operaciones a hacer y por lo tanto aumentar el error de redondeo y el tiempo para resolver el problema.


El uso de computadoras en el cálculo numérico no solo conduce a errores de los antes mencionados, sino que, impulsado por alguno de los anteriores, aparece el error por resolver el problema no como se ha formulado, sino a través de alguna aproximación. Este error es causado por reemplazar un infinito(sumatoria
o integral por ejemplo) por una cantidad finita de términos La precisión finita que introduce algunos de los errores numéricos, es la que opera bajo el estándar de la IEEE (Institute of Electrical and Electronics Engineers) nro. 754, que normaliza la notación en bits de los números de punto flotante, la norma tiene cuatro grados de precisión: simple \(32 bits\), simple extendida(43 bits, no se utiliza habitualmente), doble (64) y doble extendida (implementada en 80 bits o más). Todas las normas cuentan con tres campos: signo, mantisa y exponente, los dos últimos varían su longitud según de cual se trate. En el caso de la doble extendida, se asigna un bit al signo (la parte más alta de la cadena), 64 a la mantisa (que se almacena en la parte baja de la cadena en notación sin signo) y 15 al exponente(entre las otras dos, se encuentra desplazado 2\^{}(14) ). El rango representado es [-10\^{}(4932), 10\^{}(4932)] aprox.

A su vez, existen varias maneras de cuantificar el error, de forma que su medición se torne algo más tangible y útil, entre ellas, el error relativo y el error absoluto son las que trataremos en este trabajo. Desde el punto de vista absoluto, el error surge de la diferencia que pudiera haber entre la magnitud real que se desea expresar, y la obtenida mediante cálculos computacionales. Por otra parte el error relativo tiene en cuenta no solo las magnitudes, sino el cociente entre la diferencia anterior y el valor absoluto de la magnitud medida. De esta forma se obtiene una real dimensión del error y no tan solo el valor absoluto del mismo. Por estos motivos, una de las tareas del análisis numérico se trata de buscar un algoritmo que lleve a la mejor solución posible de cada problema. De esta forma se pueden definir muchos algoritmos que lleguen a la solución, pero se seleccionará el que mejor aproxime a la solución del problema, es decir, el que mejor utilice las operaciones respecto de sus errores.

En este caso surge la noción de estabilidad numérica. La estabilidad numérica define cuan buena será la solución de nuestro problema usando métodos aproximados. Estos métodos pueden tener un resultado diferente al esperado, ya que tienen diferente estabilidad numérica, esto quiere decir que para ciertos
valores de entrada, con sus respectivos errores, el método puede propagar el error por el algoritmo en mayor o menor medida. De esta forma, el algoritmo que mejor aproxime a la solución del problema, será aquel que tenga mejor estabilidad numérica.

\medskip
\leftline{\bf El problema:}

El problema puntual abordado por este trabajo es obtener una aproximación de la función e\^{}(-x), mediante el polinomio de Taylor. El desarrollo de este algoritmo contempla dos variantes:

\begin{enumerate}
\item Desarrollar la función f(x) = e\^{}(-x)en serie de Taylor alrededor del 0, y evaluar esta serie en el punto x = a.

\item Desarrollar la función f(x) = e\^{}x en serie de Taylor alrededor del 0, evaluar esta serie en el punto x = a y responder la inversa del valor calculado.
\end{enumerate}

\leftline{En cualquier caso, se incurre en varios tipos de error, a saber:}
\begin{enumerate}
\item Error debido a la implementación en aritmética finita 
\item Error de truncamiento en cuanto a que el polinomio de Taylor es una serie, por tanto infinita, de términos, que se verá acotada a la cantidad que el usuario considere conveniente.
\end{enumerate}

Es condición para el trabajo realizar la aritmética con precisión arbitraria (no por eso pierde su calidad de finita), determinada por el usuario. Entiéndase por precisión, el número de bits de la mantisa en notación normalizada, esta cantidad esta acotada (en este trabajo) superiormente por 64.

El resultado de analizar las magnitudes y los comportamientos de los errores introducidos por el polinomio de Taylor para el cálculo de la función serán abordados en este mismo trabajo, más adelante.

\medskip
\leftline{\bf Detalles de implementación:}


Como la condición del trabajo es obtener el valor de la función e\^{}(-x) implementando una aritmética de precisión arbitraria, comenzamos pensando cómo es que realmente íbamos a implementarla. Luego de algunas reflexiones, llegamos a concebir dos formas diferentes. La primera, diseñar e implementar desde cero, una aritmética arbitraria adaptada al problema que se quería resolver. La segunda, utilizar la aritmética fija que nos provee la PC como base y modificarla de alguna manera para que tenga la propiedad de ser arbitraria.

Comenzamos planteando la primera opción, buscando una manera de representar los datos. No hubo divergencia en cuanto a este tema, ya que solo una forma de representación nos vino a la mente, la de utilizar un arreglo de bool del tamaño que el usuario indicara para la cantidad de dígitos en la mantisa, más los del exponente y el signo. Este arreglo contendría ceros y unos, y codificaría cada número a representar, ya sea por ingreso manual del dato o como resultado de alguna función del sistema a implementar.

El formato a utilizar para almacenar los datos dentro del arreglo iba a estar basado en el estándar IEEE numero 754. Es decir, que el arreglo tendría tres partes, una parte para el signo, otra para el exponente y otra para la mantisa. En principio, un problema que se nos planteo era el de saber o determinar cuantos "bits" tendría asignado el exponente, el signo y la mantisa no tendrían ningún inconveniente, ya que tendrían un bit y la cantidad ingresada por el usuario respectivamente. Como nuestra idea era conseguir una mejor precisión a la hora de resolver el problema, decidimos asignarle la misma cantidad de bits que usa el estándar IEEE numero 754 en su versión extendida, ya que consideramos esa cantidad ni muy pobre ni muy excesiva a la hora de hacer los cálculos.

Finalmente, dada la envergadura de la implementación que esta opción requriría, la consideramos demasiado compleja, por lo que desistimos de utilizarla.

Planteamos la segunda opción, y de inmediato nos percatamos de que era relativamente mas fácil de implementar que la anterior, pero acarreaba el inconveniente de que la cantidad de bits en la mantisa estaba limitada superiormente. Esto nos permitió saber que íbamos a tener una cota con respecto a la precisión que podría requerir el usuario. De todas formas, a pesar del inconveniente mencionado, nos pareció mas seguro con respecto a los cálculos, trabajar con una representación que ya estaba asentada y por lo tanto funcionando.


Teniendo en cuenta estas consideraciones sobre la complejidad de implementación y confiabilidad de la aritmética nos decidimos finalmente por la segunda opción.

Utilizando el tipo de dato long double (que tiene un formato de 80 bits significativos, con las características del IEEE 754 doble extendido mencionados más arriba) de C++  como base, creamos una clase que cuenta con dos atributos: el valor real, almacenado en un long double, y un entero que nos indica la precisión con la que se van a enmascarar los datos. El nombre de la clase es DLFloat (Defined Longitude Float).

\leftline{La clase funciona de la siguiente manera:}

\begin{enumerate}
\item Se crea un DLFloat con una precisión y un valor decididos por el usuario. El valor, inicialmente long double, se trunca para acordarse con la precisión. Los valores por defecto, en el caso que el usuario no ingrese un parametro, son: cero para el valor (formato IEEE 754 doble extendido) y 63 de precisión. Caso contrario, la precision debe ser mayor a 0 y menor que 64.


\item Cada operación se efectúa en precisión doble extendida, y se trunca para mantener la precisión. Las operaciones disponibles son: suma, resta, multiplicación, división y potenciación. En el caso particular de la división se incluyó por comodidad la división contra un long double, para todo el resto de las operaciones los operandos son siempre DLFloat.


\item Toda operación aritmetica se realiza con precisión doble extendida y finaliza con un llamado a la función "truncar", ésta se encarga de acotar la precisión del resultado obtenido mediante la manipulación en 80 bits de los operandos a la requerida por el usuario. El método que sigue esta función consiste en reservar los bits más significativos hasta completar la precisión requerida, y colocar ceros en los bits que sobrepasan la precision pedida. Tras efectuar el truncamiento, tanto el signo como el exponente conservan su valor.
\end{enumerate}

En cuanto a la función 'truncar', cabe aclarar que durante gran parte del desarrollo se mantuvo una función llamada 'redondear', que seguía el mismo método que la anterior, pero sumaba uno en el bit más significativo de los que  serían descartados. Esto acarreó varios problemas, en primer lugar, por la forma de la implementación la suma no se propagaba a través de los bytes. Cuando solucionamos ese inconveniente surgió otro, si la mantisa del número consistía solo en unos, el redondeo se propagaba hasta el exponente (comportamiento matemáticamente correcto) y la mantisa quedaba en cero. Este formato es indefinido para los compiladores utilizados, y si bien en el caso de las potencias inversas de e, obtenerlo resulta imposible, se opto por el truncamiento en pos de una implementación más firme y reutilizable. 


Habiéndonos decidido en cuanto a la representación de los valores que debíamos manejar, pasamos a trasladar el algoritmo de la serie de Taylor a la PC  en las condiciones antes mencionadas.

Esta implementación no revistió mayores dificultades, razón por la cual decidimos agregar dos versiones más a las ya requeridas, teniendo en total cuatro versiones del algoritmo que implementa el polinomio de Taylor. Las dos primeras son las mencionadas en el enunciado y reproducen fielmente la serie de Taylor, definida matemáticamente, hasta una cantidad de términos decidida por el usuario. Las otras dos, alteran la sucesión de los términos de manera que la suma de los mismos se efectúa de manera inversa, yendo desde los términos más pequeños hacia los más grandes.

Establecidas las cuatro formas de aproximar e^(-x), las hipótesis preliminares acerca del error situaban a la serie de Taylor de e^(-x) calculada de menor a mayor como la de mejor rendimiento. En segundo lugar, pero sin unanimidad, se encontraban la serie de Taylor de e^(x) invertida de mayor a menor, y la variante e^(x) de menor a mayor. En último lugar, todos estuvimos de acuerdo en colocar a la serie para e^(x) invertida y calculada de mayor a menor.

Los criterios para establecer estas hipótesis son básicamente dos: si se suman primero los números más pequeños, estos se vuelven los suficientemente significativos como para poder modificar a un número más grande en una suma. De otro modo puede suceder que sumar un número grande a uno pequeño, nos dé por resultado el grande sin modificación.

El segundo criterio es muy sencillo, consideramos que la serie de Taylor para e^(-x), aproxima mejor a ese número que la de e^(x) invertida, ya que la primera es la serie "exacta", mientras que la segunda no lo es, y para transformarla se requiere una operación extra, que incrementa el error.

\medskip
\leftline{\bf Resultados:}

Se hicieron diversas pruebas, en las cuales se probo la calidad del resultado a calcular. Estas consisten en: Mostrar el error relativo para cada método en función de la cantidad de iteraciones de la serie de Taylor; Mostrar el error relativo en cada método en función de la precisión requerida por el usuario; Y comparar el error relativo de los métodos en considerando el valor en el que se instanciaría la función




\leftline{Orden vs Precision:}

En los dos primero gráficos se muestran lo errores relativos calculando e\^{}(-x) y 1/e\^{}(x), los dos en función de la precision ingresada, para un punto de evaluación x=1 y para un orden de la serie de Taylor de 50 términos.

\centerline{\includegraphics[scale=0.5]{grafico1.jpg}}

\centerline{\scriptsize Grafico1}

En el grafico 1 se ve reflejado el error relativo usando la versión de la serie de Taylor que calcula desde los términos mas chicos en valor a los mas grandes. En este caso se ve una clara diferencia entre e\^{}(-x) y 1/e\^{}x al comienzo del grafico, luego a medida que aumenta la precisión, el error relativo de ambos métodos se va acercando a cero rápidamente. De todas formas, según el gráfico la función que menos error relativo tiene a medida que aumenta la precisión es 1/e\^{}x.

\centerline{\includegraphics[scale=0.5]{grafico2.jpg}}

\centerline{\scriptsize Grafico2}

En este otro gráfico se refleja la misma situación antes comentada, con la diferencia que se usa la variante de Taylor que comienza por los términos más grandes en tamaño hacia los más chicos. En este caso se percibe al comienzo del gráfico, que ambos métodos tienen un error relativo mucho menor con respecto a la otra serie. Al igual que en el gráfico anterior, el método que menor error relativo tiene es 1/e\^{}x.

En los dos siguientes gráficos se mostrará el error relativo en función del orden de la serie de Taylor(es decir la cantidad de términos a desarrollar).

\centerline{\includegraphics[scale=0.5]{grafico3.jpg}}

\centerline{\scriptsize Grafico3}

En el grafico 3 se ve claramente como a partir del quinto orden, el error relativo se hace muy chico, para ambas series de Taylor. En este juego de gráficos la serie de menor error relativo es nuevamente 1/e\^{}x.

\centerline{\includegraphics[scale=0.5]{grafico4.jpg}}

\centerline{\scriptsize Grafico4}

Este caso es claramente peor que el anterior ya que para una cantidad pequeña de términos de la serie, el error que se comente es mucho mayor.
En ambos gráficos se nota un mejor desempeño para las series calculadas de menor a mayor, esto se debe, creemos, al criterio expresado anteriormente, según el cual este método de sumar los términos, tiende a brindarle significatividad a cada uno de los operandos.

\centerline{\includegraphics[scale=0.5]{grafico5.jpg}}

\centerline{\scriptsize Grafico5}

En los últimos 2 gráficos, en donde varia el punto de evaluación y permanecen fijos el orden de la serie y la precisión, se ve como para ciertos valores un método aproxima mejor que otro, esto quiere decir que el error relativo para ciertos valores es mucho menor en un método que en otro. Por ejemplo, para los valores positivos (x > 0) se observa que ambos tienen un error relativo pequeño hasta el valor 15 aproximadamente, luego de ese valor los 2 métodos se comportan de manera similar cuando el valor a evaluar es muy grande, salvo entre los valores 15 y 35 aproximadamente donde se observa claramente que 1/e\^{}x tiene una mejor aproximación con respecto a e\^{}(-x).

Para los valores negativos, ocurre algo diferente. Hasta el valor -15 aproximadamente ambos métodos tiene un error pequeño. Pero entre los valores -15
y -35 vemos que pasa lo inverso a lo que pasaba en los valores positivos, en este caso el método para e\^{}(-x) mantiene un error relativo pequeño respecto del de 1/e\^{}x, por lo tanto se obtiene un mejor resultado. Para los valores menores que -35, se observa que ambos métodos convergen en un error igual a 1, 
lo que indica que no se obtiene una buena aproximación para esos valores. 

\centerline{\includegraphics[scale=0.5]{grafico6.jpg}}

\centerline{\scriptsize Grafico6}

Este último gráfico es muy similar al anterior, con la diferencia que con la versión de la serie de Taylor utilizada en este gráfico, se observa una leve perdida en la precisión, además del pico que se observa entre los valores -15 y -20 aproximadamente para 1/e\^{}x.
No es despreciable notar en los gráficos 5 y 6 que para valores altos de x el error es no acotado, tiende a infinito. Este fenómeno es una consecuencia de la utilización de la aritmética de precisión finita. Con esta forma de representación para los valores, aquellos x por sobre un valor determinado se salen del rango aceptado, haciendo que la PC busque representaciones alternativas y que el error relativo sea muy grande.

Como conclusión general resultante de todos los gráficos, los métodos que emplean la inversa de e^x son con mucho, más precisos que los que se basan directamente en e^(-x). Esto da por tierra con una de nuestras hipótesis preliminares, sin embargo, tiene un asidero teórico. La segunda variante es una serie de sumas y restas, esta última operación esta mal condicionada, lo que  quiere decir que para valores similares incurre en grandes errores. Justamente la serie de Taylor guarda cierta similaridad entre sus valors contiguos, con lo que podríamos considerarla un caso patologico para el mal condicionamiento de la resta, con lo que esta variante pierde confiabilidad.     


\end{document}
