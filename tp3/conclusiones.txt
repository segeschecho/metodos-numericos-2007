Conclusiones:

Con respecto a lo observado podemos sacar las siguientes conclusiones: en las pruebas de error cuadrático medio se vio un resultado menor para imágenes que contienen valores más bajos, más cercanos a 0 (colores más oscuros, en escala de grises), esto se debe al cálculo que se realiza para obtener los tiempos, ya que al hacer las cuentas y tener una velocidad (valor del pixel) muy grande, la división se vuelve chica por lo cual se esta más propenso a errores grandes de redondeo, en contraste con un número calculado con un menor valor de la velocidad. Esto implica tener los datos con un cierto nivel de error, que al momento de hacer la reconstrucción, se suma al que ya es propio de la aproximación.

Con esto se puede inferir lo siguiente: imágenes (en escala de grises) con mayoría de pixels oscuros, tienen más oportunidad de generar error adicional al cometido durante la reconstrucción. Sin embargo, teniendo un método de distribución de señales eficiente, esto no se vuelve un problema grave, lo que es seguro, es que su incidencia en el resultado final debe ser tenido en cuenta.


Siguendo con los patrones de distribución, se observó durante el trabajo que no siempre el mejor método para reconstruir una imagen es el que llega a atravezar todos los puntos de la discretizacion, si no el que puede surcar cada punto con más de una recta y además en diferentes direcciones, la mayor cantidad que sea posible, en función de recopilar la mayor cantidad de datos posibles para cada sector a reconstruir. En la sección de resultados se puede observar que el método 3 no es nada eficiente a pesar que genera un número importante de señales. Esto se debe a que la distribución de la emisión de señales no es óptima por lo explicado anteriormente. En síntesis, se puede concluir que el resultado a obtener esta directamente relacionado (entre otras cosas) con el tipo de distribucion de estas señales, y que una mala elección de esta puede llevar a resultados incongurentes con lo esperado y por lo tanto incorrectos.